{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Session 5: Gradient Descent \n",
    "\n",
    "## House sale-value prediction using the Boston housing dataset\n",
    "\n",
    "------------------------------------------------------\n",
    "*ATDST, 2017-2018*\n",
    "\n",
    "*Pablo M. Olmos olmos@tsc.uc3m.es*\n",
    "\n",
    "------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "from sklearn import linear_model\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Today, we will continue with the example we used for session 2: Predicting house values using the average number of rooms in the [Boston housing dataset](https://archive.ics.uci.edu/ml/machine-learning-databases/housing/housing.names).\n",
    "\n",
    "Attribute Information can be found [here](https://www.kaggle.com/c/boston-housing).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Data\n",
    "\n",
    "We will manage the database using [Panda's library and Dataframes](http://pandas.pydata.org/pandas-docs/stable/tutorials.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_data=pd.read_csv('./Boston_train.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We divide the whole data set in **80% training** and **20% test**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = len(housing_data)\n",
    "\n",
    "N_train = np.round(0.8 * N).astype(np.int32)\n",
    "\n",
    "np.random.seed(seed=10) #To fix the random seed\n",
    "\n",
    "mask = np.random.permutation(len(housing_data))\n",
    "\n",
    "\n",
    "regression_data_frame = housing_data[['rm','medv']].iloc[list(mask[0:N_train])]\n",
    "\n",
    "X_0 = np.array(regression_data_frame['rm']) \n",
    "Y = np.array(regression_data_frame['medv'])\n",
    "\n",
    "regression_data_frame_test = housing_data[['rm','medv']].iloc[list(mask[N_train:-1])]\n",
    "\n",
    "X_0_test = np.array(regression_data_frame_test['rm']) \n",
    "Y_test = np.array(regression_data_frame_test['medv'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Â Numerical Optimization with Gradient Descend\n",
    "\n",
    "As we know, the Ridge regression optimization problem\n",
    "$$\\boldsymbol{\\theta}_\\lambda = \\arg \\min_{\\theta} \\frac{1}{N} \\left[\\sum_{i=1}^{N} (y^{(i)}-\\boldsymbol{\\theta}^T\\mathbf{x}^{(i)})^2 + \\lambda \\sum_{j=1}^{D+1} \\theta_j^2\\right],$$\n",
    "can be solved using the **normal equation**\n",
    "$$\\boldsymbol{\\theta}_\\lambda = (\\mathbf{X}^T\\mathbf{X} + \\mathbf{D}_\\lambda)^{-1}\\mathbf{X}^T\\mathbf{y},$$\n",
    "\n",
    "However, as we advanced in Session 2, computing the normal equation has many important drawbacks:\n",
    "- You need to keep the full target matrix $\\mathbf{X}_{N\\times (D+1)}$ in memory, which can be huge in large datasets!\n",
    "- You need to invert the matrix $(\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}$ $\\Rightarrow$ $\\mathcal{O}(D^3)$ complexity.\n",
    "- For small data sets ($N\\leq D$), $(\\mathbf{X}^T\\mathbf{X})^{-1}$ can be non-invertible!\n",
    "- Once you get new data, how do you update $\\boldsymbol{\\theta}^*$?\n",
    "\n",
    "Today, we will learn how to apply a very simple algorithm to perform numerical optimization: **gradient descent** (GD). GD is one of the cornerstones of **convex optimization**, but it is also widely used for **non-convex optimization** for instance in [deep learning with neural networks](https://www.youtube.com/watch?v=IHZwWFHWa-w). Be aware that GD is probably the simplest numerical optimization algorithm  and that there is a whole [field](https://web.stanford.edu/group/sisl/k12/optimization/#!index.md) devoted to optimize numerically a function.\n",
    "\n",
    "In a nutshell, GD tries to iteratively converge to the minimum of a function $f(\\boldsymbol{\\theta})$ by iteratively applying the following update rule:\n",
    "\n",
    "$$\\boldsymbol{\\theta}_{\\ell+1} = \\boldsymbol{\\theta}_\\ell - \\alpha \\nabla f(\\boldsymbol{\\theta})|_{\\boldsymbol{\\theta}_\\ell},$$\n",
    "\n",
    "where $\\nabla(\\cdot)$ is the gradient operator and $\\alpha$ is the learning rate. **Setting the learning rate** is a problem in general. Despite we won't cover the topic in detail and just restrict to simple checks, be aware that there are many modifications to GD that attempt to authomatically tune $\\alpha$, like the [line search](https://people.maths.ox.ac.uk/hauser/hauser_lecture2.pdf) method.\n",
    "\n",
    "\n",
    "#### Check out this [beautiful post](http://www.benfrederickson.com/numerical-optimization/)!\n",
    "\n",
    "___\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solving the Ridge regression problem using GD\n",
    "\n",
    "If we define\n",
    "\n",
    "$$ J(\\boldsymbol{\\theta}) = \\frac{1}{N} \\left[\\sum_{i=1}^{N} (y^{(i)}-\\boldsymbol{\\theta}^T\\mathbf{x}^{(i)})^2 + \\lambda \\sum_{j=1}^{D+1}\\theta_j^2\\right],$$\n",
    "\n",
    "then it is easy to check that, for $m>0$:\n",
    "\n",
    "\\begin{align}\n",
    "\\frac{\\partial J(\\boldsymbol{\\theta})}{\\partial \\theta_m}= \\frac{2}{N} \\left[\\lambda\\theta_m-\\sum_{i=1}^{N} x_m^{(i)}\\left(y^{(i)}-\\boldsymbol{\\theta}^T\\mathbf{x}^{(i)}\\right)\\right]= \\frac{2}{N} \\left[\\lambda\\theta_m - \\mathbf{e}^T \\mathbf{X}_{:,m}\\right],\n",
    "\\end{align}\n",
    "where $\\mathbf{e}=\\mathbf{y}-\\boldsymbol{\\theta}^T \\mathbf{X}$ is the error vector and $\\mathbf{X}_{:,m}$ is the $m$-th column of the normalized training feature matrix. For $m=0$ the first term is not present, since the intercept is not regularized:\n",
    "\\begin{align}\n",
    "\\frac{\\partial J(\\boldsymbol{\\theta})}{\\partial \\theta_m}= \\frac{-2}{N} \\left[\\sum_{i=1}^{N} x_m^{(i)}\\left(y^{(i)}-\\boldsymbol{\\theta}^T\\mathbf{x}^{(i)}\\right)\\right]= \\frac{-2}{N} \\left[\\mathbf{e}^T \\mathbf{X}_{:,m}\\right],\n",
    "\\end{align}\n",
    "\n",
    "Note that in both cases ** the gradient vanishes when the error is zero**.\n",
    "\n",
    "Lets now program the GD method in a modular way. First, we incorporate some useful functions from the ** previous session**. They will be included as a Python library file.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import Ridge_functions as ridge   # You can find details in provided file S4_Ridge_functions.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute Gradient\n",
    "\n",
    "** First ** you have to program a function that computes the gradient and takes as input arguments\n",
    "- The feature matrix $\\mathbf{X}$\n",
    "- The error vector $\\mathbf{e}$\n",
    "- The value of $\\lambda$\n",
    "- The current value of $\\boldsymbol{\\theta}$\n",
    "and returns as output the $(D+1)$-dimensional gradient of $J(\\boldsymbol{\\theta})$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_gradient(feature_matrix,error,l,T):\n",
    "    ## YOUR CODE HERE\n",
    "    \n",
    "    gradient = l*T - error @ feature_matrix\n",
    "    gradient[0] -= l*T[0]\n",
    "    gradient *= 2.0/feature_matrix.shape[0]\n",
    "    \n",
    "    return(gradient)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Numerical evaluation of the gradient\n",
    "\n",
    "Before moving forward, let's run a numerical check to verify that the gradient you compute in the function above is correct.\n",
    "\n",
    "1) Fix a given arbitrary value of $\\boldsymbol{\\theta}^o$.\n",
    "\n",
    "2) Introduce a small distortion in one of the components of $\\boldsymbol{\\theta}$. For instance, $\\theta_m^+=\\theta^o_m$ for $m\\neq 1$ and $\\theta^+_1 = \\theta^o_1+\\epsilon$, where $\\epsilon=10^{-3}$. Evaluate the cost function $J(\\boldsymbol{\\theta}^+)$.\n",
    "\n",
    "3) Define a new vector $\\boldsymbol{\\theta}^-$ such that $\\theta_m^-=\\theta^o_m$ for $m\\neq 1$ and $\\theta^-_1 = \\theta^o_1-\\epsilon$, where $\\epsilon=10^{-3}$. Evaluate the cost function $J(\\boldsymbol{\\theta}^-)$.\n",
    "\n",
    "4) Verify that \n",
    "\n",
    "$$\\frac{\\partial J(\\boldsymbol{\\theta})}{\\partial \\theta_1} \\approx \\frac{J(\\boldsymbol{\\theta}^+)-J(\\boldsymbol{\\theta}^-)}{2\\epsilon}$$,\n",
    "where the derivative in the left hand side is given by the second component of the gradient computed using your function above. **Repeat the experiment using the first component of the gradient, i.e., the derivative w.r.t. $\\theta_0$**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The gradient at position 1 is -12.406842\n",
      "The approximate gradient at position 1 is -12.406842\n"
     ]
    }
   ],
   "source": [
    "# Lets fix a degree\n",
    "deg = 2\n",
    "l= 1.0\n",
    "np.random.seed(seed=10) #To fix the random seed\n",
    "\n",
    "T = np.random.rand(3)\n",
    "\n",
    "# This is how we would compute the cost function at T and obtain the normalized train feature matrix\n",
    "# J_train,_,F_train,_ = ridge.eval_J_Ridge_given_T(X_0,X_0_test,deg,Y,Y_test,l,T)\n",
    "\n",
    "# Your code here\n",
    "mod = np.zeros(T.shape)\n",
    "epsilon = 1e-2\n",
    "index = 0\n",
    "mod[index] += epsilon\n",
    "\n",
    "T_p = T + mod\n",
    "T_m = T - mod\n",
    "\n",
    "J_p,_,F_train,_ = ridge.eval_J_Ridge_given_T(X_0,X_0_test,deg,Y,Y_test,l,T_p)\n",
    "J_m,_,_,_ = ridge.eval_J_Ridge_given_T(X_0,X_0_test,deg,Y,Y_test,l,T_m)\n",
    "\n",
    "error = (Y - ridge.LS_evaluate(F_train,T))\n",
    "\n",
    "gradient = compute_gradient(F_train,error,l,T)\n",
    "\n",
    "print(\"The gradient at position %d is %f\" %(index,gradient[index]))\n",
    "\n",
    "print(\"The approximate gradient at position %d is %f\" %(index,(J_p-J_m)/(2*epsilon))) # Your code here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing GD\n",
    "\n",
    "Now, complete the following function to estimate the Ridge solution using gradient descend. The inputs are:\n",
    "\n",
    "- The normalized train feature matrix $\\mathbf{X}$\n",
    "- The target vector $\\mathbf{Y}$\n",
    "- The initial value of theta $\\boldsymbol{\\theta}$\n",
    "- The step size $\\alpha$\n",
    "- The stopping tolerance: a threshold that halts the algorithm when the norm of the gradient is below this limit\n",
    "- The maximum number of iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regression_gradient_descent(F_train, Y, T_0, l, step_size, tolerance, iter_max,verbose=True,period_verbose=1000):\n",
    "    converged = False \n",
    "    T = np.array(T_0) # make sure it's a numpy array\n",
    "    it=0\n",
    "    \n",
    "    while not converged:\n",
    "\n",
    "        # First, compute the error vector \n",
    "        error= (Y - ridge.LS_evaluate(F_train,T)) #YOUR CODE HERE\n",
    "        \n",
    "        # Second compute the gradient vector\n",
    "        gradient= compute_gradient(F_train,error,l,T) #YOUR CODE HERE\n",
    "        \n",
    "        # Finally, update the theta vector\n",
    "        T=T- step_size * gradient #YOUR CODE HERE\n",
    "        \n",
    "        grad_norm=np.linalg.norm(gradient)\n",
    "        \n",
    "        if(verbose==True and it % period_verbose ==0):\n",
    "            J_Ridge = ridge.J_error_L2(Y,ridge.LS_evaluate(F_train,T),T,l)  \n",
    "            print (\"Iterations = %d\" %(it))\n",
    "            print (\"Gradient norm %f\" %(grad_norm))\n",
    "            print (\"Ridge cost function %f\" %(J_Ridge))            \n",
    "\n",
    "        if grad_norm < tolerance:\n",
    "            converged = True\n",
    "        elif it > iter_max:\n",
    "            converged = True\n",
    "        else:\n",
    "            it=it+1\n",
    "        \n",
    "    if(converged==True):\n",
    "        J_Ridge = ridge.J_error_L2(Y,ridge.LS_evaluate(F_train,T),T,l)  \n",
    "        print (\"Iterations = %d\" %(it))\n",
    "        print (\"Gradient norm %f\" %(grad_norm))\n",
    "        print (\"Ridge cost function %f\" %(J_Ridge))\n",
    "        \n",
    "    return(T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate solution and compare with normal equation\n",
    "\n",
    "Run an example to verify that the GD solution is close to the one predicted by the normal equation. Investigate the effect of the step size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iterations = 0\n",
      "Gradient norm 50.330715\n",
      "Ridge cost function 527.205810\n",
      "Iterations = 10000\n",
      "Gradient norm 0.034596\n",
      "Ridge cost function 33.633354\n",
      "Iterations = 20000\n",
      "Gradient norm 0.014153\n",
      "Ridge cost function 33.577614\n",
      "Iterations = 30000\n",
      "Gradient norm 0.005790\n",
      "Ridge cost function 33.568285\n",
      "Iterations = 40000\n",
      "Gradient norm 0.002369\n",
      "Ridge cost function 33.566724\n",
      "Iterations = 49655\n",
      "Gradient norm 0.001000\n",
      "Ridge cost function 33.566466\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x1a1810f470>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXYAAAD8CAYAAABjAo9vAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nHkaCXQghPIwEuxBCeBgJdiGE8DD/D6QN2cXTHdfaAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a14e0f4a8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Lets fix a degree\n",
    "deg = 5\n",
    "l= 1.0\n",
    "T = np.random.rand(6)\n",
    "\n",
    "#Cost function at initial T and feature matrix\n",
    "J_0,_,F_train,_ = ridge.eval_J_Ridge_given_T(X_0,X_0_test,deg,Y,Y_test,l,T)\n",
    "\n",
    "step_size = 1e-02\n",
    "iter_max = 5e4\n",
    "tolerance = 1e-03\n",
    "period= 1e04\n",
    "\n",
    "T_opt = regression_gradient_descent(F_train, Y, T, l, step_size, tolerance, iter_max,verbose=True,period_verbose=period)\n",
    "\n",
    "T_normal = ridge.Ridge_solution(F_train,Y,l)\n",
    "\n",
    "plt.stem(T_normal,'r',label='Normal Equation')\n",
    "plt.plot(T_opt,'b',label='Gradient Descent')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
