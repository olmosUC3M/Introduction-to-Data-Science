{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Session 15: Image classification with Convolutional NNs\n",
    "\n",
    "------------------------------------------------------\n",
    "*Introduction to Data Science & Machine Learning*\n",
    "\n",
    "*Pablo M. Olmos olmos@tsc.uc3m.es*\n",
    "\n",
    "------------------------------------------------------\n",
    "\n",
    "Goals for today:\n",
    "    - Train an MNIST image classifier based on CNNs\n",
    "   \n",
    "\n",
    "\n",
    "This is a personal wrap-up of all the material provided by [Google's Deep Learning course on Udacity](https://www.udacity.com/course/deep-learning--ud730), so all credit goes to them. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# On convolutional NNs\n",
    "\n",
    "- Check out the [following slides](http://www.ucsp.edu.pe/ciet/mlss16/file/shlens.pdf)\n",
    "\n",
    "- The following [paper](https://arxiv.org/pdf/1603.07285v1.pdf) is very nice to understand the arithmetic of convolution operations in 2D\n",
    "\n",
    "- Check out the following video for a quick introduction to CNNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/jpeg": "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\n",
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"400\"\n",
       "            height=\"300\"\n",
       "            src=\"https://www.youtube.com/embed/jajksuQW4mc\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.YouTubeVideo at 0x108c0aa20>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import YouTubeVideo\n",
    "\n",
    "YouTubeVideo('jajksuQW4mc') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from six.moves import cPickle as pickle\n",
    "from six.moves import range\n",
    "from IPython.display import display, Image\n",
    "import matplotlib.pyplot as plt\n",
    "# Config the matlotlib backend as plotting inline in IPython\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.3.0\n"
     ]
    }
   ],
   "source": [
    "# Lets check what version of tensorflow we have installed. The provided scripts should run with tf 1.0 and above\n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading MNIST database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/olmos/anaconda3/lib/python3.6/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_mldata\n",
    "from sklearn.cross_validation import train_test_split\n",
    "mnist = fetch_mldata('MNIST original')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "images = mnist.data.astype(np.float32)\n",
    "labels = mnist.target\n",
    "\n",
    "train_dataset,test_dataset,train_labels,test_labels = train_test_split(images, labels, test_size=0.33, random_state=42)\n",
    "\n",
    "train_dataset,valid_dataset,train_labels,valid_labels = train_test_split(train_dataset, train_labels, test_size=0.33, random_state=42)\n",
    "\n",
    "# Separate a smaller data set for training (20000 images) \n",
    "N = 20000\n",
    "\n",
    "train_dataset = train_dataset[:N,:]\n",
    "train_labels = train_labels[:N]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reformat labels as 1-hot encodings and images into tensors\n",
    "\n",
    "**Take care! Input image set is now a 4-dimensional tensor! num_images $\\times$ image_width $\\times$ image_height $\\times$ num_channels**\n",
    "\n",
    "For gray-scale MNIST images, the number of imput channels is one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (20000, 28, 28, 1) (20000, 10)\n",
      "Validation set (15477, 28, 28, 1) (15477, 10)\n",
      "Test set (23100, 28, 28, 1) (23100, 10)\n"
     ]
    }
   ],
   "source": [
    "image_size = 28\n",
    "num_labels = 10\n",
    "num_channels = 1 # grayscale\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def reformat(dataset, labels):\n",
    "    dataset = dataset.reshape((-1, image_size, image_size, num_channels)).astype(np.float32)\n",
    "    labels = (np.arange(num_labels) == labels[:,None]).astype(np.float32)\n",
    "    return dataset, labels\n",
    "\n",
    "train_dataset, train_labels = reformat(train_dataset, train_labels)\n",
    "valid_dataset, valid_labels = reformat(valid_dataset, valid_labels)\n",
    "test_dataset, test_labels = reformat(test_dataset, test_labels)\n",
    "print('Training set', train_dataset.shape, train_labels.shape)\n",
    "print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def accuracy(predictions, labels):\n",
    "    return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))\n",
    "          / predictions.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Let's build a small network with two convolutional layers, followed by one fully connected layer. Convolutional networks are more expensive to train computationally, so we'll limit its depth and number of fully connected nodes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lets train our first CNN\n",
    "\n",
    "Two 5x5 convolutional layers with stride 2 and Relu non-linear layers, followed by a fully connected year with soft-max.\n",
    "\n",
    "<img src=\"CNN1.png\">\n",
    "\n",
    "The number of parameters of the network is:\n",
    "\n",
    "- First convolutional Layer: 1 x 16 x 5 x 5 = 400 parameters (+16 of bias)\n",
    "- Second convolutional Layer: 16 x 16 x 5 x 5 = 6400 parameters (+16 of bias)\n",
    "- Fully Connected Layer: 224 x 64 = 14336 parameters (+64 of bias)\n",
    "- Last fully connected layer: 640 parameters (+10 of bias)\n",
    "\n",
    "** Total: 21882 parameters **\n",
    "\n",
    "The following code implements the above structure step by step. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "patch_size = 5\n",
    "depth = 16\n",
    "num_hidden = 64\n",
    "\n",
    "graphCC = tf.Graph()\n",
    "\n",
    "with graphCC.as_default():\n",
    "\n",
    "    # Input data.\n",
    "    tf_train_dataset = tf.placeholder(\n",
    "        tf.float32, shape=(batch_size, image_size, image_size, num_channels))\n",
    "    tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "    tf_valid_dataset = tf.constant(valid_dataset)\n",
    "    tf_test_dataset = tf.constant(test_dataset)\n",
    "  \n",
    "    # Variables.\n",
    "    layer1_weights = tf.Variable(tf.truncated_normal(\n",
    "          [patch_size, patch_size, num_channels, depth], stddev=0.1))\n",
    "    layer1_biases = tf.Variable(tf.zeros([depth]))\n",
    "    \n",
    "    layer2_weights = tf.Variable(tf.truncated_normal(\n",
    "      [patch_size, patch_size, depth, depth], stddev=0.1))\n",
    "    \n",
    "    layer2_biases = tf.Variable(tf.constant(1.0, shape=[depth]))\n",
    "    \n",
    "    layer3_weights = tf.Variable(tf.truncated_normal(\n",
    "      [image_size // 4 * image_size // 4 * depth, num_hidden], stddev=0.1)) ##Stride =2 !!\n",
    "    \n",
    "    layer3_biases = tf.Variable(tf.constant(1.0, shape=[num_hidden]))\n",
    "    \n",
    "    layer4_weights = tf.Variable(tf.truncated_normal(\n",
    "          [num_hidden, num_labels], stddev=0.1))\n",
    "    layer4_biases = tf.Variable(tf.constant(1.0, shape=[num_labels]))\n",
    "    \n",
    "    # Model.\n",
    "    def model(data):\n",
    "        conv = tf.nn.conv2d(data, layer1_weights, [1, 2, 2, 1], padding='SAME')\n",
    "        hidden = tf.nn.relu(conv + layer1_biases)\n",
    "        conv = tf.nn.conv2d(hidden, layer2_weights, [1, 2, 2, 1], padding='SAME')\n",
    "        hidden = tf.nn.relu(conv + layer2_biases)\n",
    "        shape = hidden.get_shape().as_list()\n",
    "        reshape = tf.reshape(hidden, [shape[0], shape[1] * shape[2] * shape[3]])\n",
    "        hidden = tf.nn.relu(tf.matmul(reshape, layer3_weights) + layer3_biases)\n",
    "        return tf.matmul(hidden, layer4_weights) + layer4_biases   \n",
    "    \n",
    "    # Training computation.\n",
    "    logits = model(tf_train_dataset)\n",
    "    loss = tf.reduce_mean(\n",
    "        tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels,logits=logits))\n",
    "    \n",
    "    # Optimizer.\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=5e-03).minimize(loss) \n",
    "    \n",
    "    # Predictions for the training, validation, and test data.\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "    valid_prediction = tf.nn.softmax(model(tf_valid_dataset))\n",
    "    test_prediction = tf.nn.softmax(model(tf_test_dataset))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 33.329277\n",
      "Minibatch accuracy: 7.0%\n",
      "Validation accuracy: 22.8%\n",
      "Minibatch loss at step 100: 0.458786\n",
      "Minibatch accuracy: 85.9%\n",
      "Validation accuracy: 90.0%\n",
      "Minibatch loss at step 200: 0.136763\n",
      "Minibatch accuracy: 96.1%\n",
      "Validation accuracy: 93.7%\n",
      "Minibatch loss at step 300: 0.104762\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 94.7%\n",
      "Minibatch loss at step 400: 0.064228\n",
      "Minibatch accuracy: 97.7%\n",
      "Validation accuracy: 95.0%\n",
      "Minibatch loss at step 500: 0.032896\n",
      "Minibatch accuracy: 98.4%\n",
      "Validation accuracy: 94.9%\n",
      "Minibatch loss at step 600: 0.032589\n",
      "Minibatch accuracy: 98.4%\n",
      "Validation accuracy: 95.0%\n",
      "Minibatch loss at step 700: 0.031264\n",
      "Minibatch accuracy: 98.4%\n",
      "Validation accuracy: 95.9%\n",
      "Minibatch loss at step 800: 0.050192\n",
      "Minibatch accuracy: 99.2%\n",
      "Validation accuracy: 95.7%\n",
      "Minibatch loss at step 900: 0.112647\n",
      "Minibatch accuracy: 97.7%\n",
      "Validation accuracy: 95.5%\n",
      "Minibatch loss at step 1000: 0.053108\n",
      "Minibatch accuracy: 99.2%\n",
      "Validation accuracy: 96.2%\n",
      "Test accuracy: 96.2%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 1001\n",
    "\n",
    "with tf.Session(graph=graphCC) as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    print('Initialized')\n",
    "    for step in range(num_steps):\n",
    "        offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "        batch_data = train_dataset[offset:(offset + batch_size), :, :, :]\n",
    "        batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "        feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "        _, l, predictions = session.run(\n",
    "              [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "        if (step % 100 == 0):\n",
    "            print('Minibatch loss at step %d: %f' % (step, l))\n",
    "            print('Minibatch accuracy: %.1f%%' % accuracy(predictions, batch_labels))\n",
    "            print('Validation accuracy: %.1f%%' % accuracy(\n",
    "                valid_prediction.eval(), valid_labels))\n",
    "        \n",
    "    print('Test accuracy: %.1f%%' % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A more complicated CNN: Le-Net 5\n",
    "\n",
    "The goal is to implement the following network\n",
    "\n",
    "<img src=\"CNN2.png\">\n",
    "\n",
    "The number of parameters of the network is:\n",
    "\n",
    "- First convolutional Layer: 1 x 16 x 5 x 5 = 400 parameters (+16 of bias)\n",
    "- Second convolutional Layer: 16 x 64 x 5 x 5 = 25600 parameters (+64 of bias)\n",
    "- **Fully Connected Layer:  3136 x 128 = 401408 parameters (+128 of bias)**\n",
    "- Fully Connected Layer: 128 x 128 = 16384 parameters (+128 of bias)\n",
    "- Last fully connected layer: 1280 parameters (+10 of bias)\n",
    "\n",
    "** Total: 445418 parameters **\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will introduce dropout and [max pooling](http://cs231n.github.io/convolutional-networks/#pool) (instead of stride 2 convolution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "patch_size = 5\n",
    "depth = 16\n",
    "depth_2 = 64\n",
    "num_hidden = 128\n",
    "drop_out = 0.5\n",
    "\n",
    "graph_CC_LeNet5_v2 = tf.Graph()\n",
    "\n",
    "with graph_CC_LeNet5_v2.as_default():\n",
    "\n",
    "    global_step = tf.Variable(0)\n",
    "    \n",
    "    # Input data.\n",
    "    tf_train_dataset = tf.placeholder(\n",
    "        tf.float32, shape=(batch_size, image_size, image_size, num_channels))\n",
    "    tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "    tf_valid_dataset = tf.constant(valid_dataset)\n",
    "    tf_test_dataset = tf.constant(test_dataset)\n",
    "  \n",
    "    # Variables.\n",
    "    layer1_weights = tf.Variable(tf.truncated_normal(\n",
    "          [patch_size, patch_size, num_channels, depth], stddev=0.1))\n",
    "    layer1_biases = tf.Variable(tf.zeros([depth]))\n",
    "    \n",
    "    layer2_weights = tf.Variable(tf.truncated_normal(\n",
    "          [patch_size, patch_size, depth, depth_2], stddev=0.1))\n",
    "    layer2_biases = tf.Variable(tf.constant(1.0, shape=[depth_2]))\n",
    "\n",
    "    #size3 = ((image_size - patch_size + 1) // 2 - patch_size + 1) // 2      #VALID PADDING, AVG_POOL=2\n",
    "    \n",
    "    size3 = image_size// 4   #SAME PADDING, AVG_POOL=2\n",
    "    \n",
    "    layer3_weights = tf.Variable(tf.truncated_normal([size3 * size3 * depth_2, num_hidden], stddev=0.1))\n",
    "    layer3_biases = tf.Variable(tf.constant(1.0, shape=[num_hidden]))\n",
    "    layer4_weights = tf.Variable(tf.truncated_normal(\n",
    "          [num_hidden, num_hidden], stddev=0.1))\n",
    "    layer4_biases = tf.Variable(tf.constant(1.0, shape=[num_hidden]))\n",
    "    layer5_weights = tf.Variable(tf.truncated_normal(\n",
    "          [num_hidden, num_labels], stddev=0.1))\n",
    "    layer5_biases = tf.Variable(tf.constant(1.0, shape=[num_labels]))    \n",
    "  \n",
    "    # Model.\n",
    "    \n",
    "    pad='SAME'\n",
    "    \n",
    "    def model_pol_2(data,prob):\n",
    "        # C1 input 28 x 28\n",
    "        conv1 = tf.nn.conv2d(data, layer1_weights, [1, 1, 1, 1], padding=pad)\n",
    "        bias1 = tf.nn.relu(conv1 + layer1_biases)\n",
    "        # S2 input 24 x 24\n",
    "        pool2 = tf.nn.max_pool(bias1, [1, 2, 2, 1], [1, 2, 2, 1], padding=pad)\n",
    "        # C3 input 12 x 12\n",
    "        conv3 = tf.nn.conv2d(pool2, layer2_weights, [1, 1, 1, 1], padding=pad)\n",
    "        bias3 = tf.nn.relu(conv3 + layer2_biases)\n",
    "        # S4 input 8 x 8\n",
    "        pool4 = tf.nn.max_pool(bias3, [1, 2, 2, 1], [1, 2, 2, 1], padding=pad)\n",
    "        # F5 input 4 x 4\n",
    "        shape = pool4.get_shape().as_list()\n",
    "        reshape = tf.reshape(pool4, [shape[0], shape[1] * shape[2] * shape[3]])\n",
    "        hidden5 = tf.nn.relu(tf.matmul(reshape, layer3_weights) + layer3_biases)\n",
    "        #F6\n",
    "        drop5=tf.nn.dropout(hidden5,prob)\n",
    "        hidden6=tf.nn.relu(tf.matmul(hidden5,layer4_weights)+layer4_biases)\n",
    "        #drop6=tf.nn.dropout(hidden6,prob)\n",
    "        return tf.matmul(hidden6, layer5_weights) + layer5_biases\n",
    "  \n",
    "    # Training computation.\n",
    "    logits = model_pol_2(tf_train_dataset,drop_out)\n",
    "    loss = tf.reduce_mean(\n",
    "        tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels,logits=logits))\n",
    "    \n",
    "    # Optimizer.\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=5e-03).minimize(loss) \n",
    "    \n",
    "    # Predictions for the training, validation, and test data.\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "    # DROP_OUT is only used in TRAINING!!!\n",
    "    valid_prediction = tf.nn.softmax(model_pol_2(tf_valid_dataset,1.))\n",
    "    test_prediction = tf.nn.softmax(model_pol_2(tf_test_dataset,1.))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 208.704529\n",
      "Minibatch accuracy: 11.7%\n",
      "Validation accuracy: 14.5%\n",
      "Minibatch loss at step 100: 0.272914\n",
      "Minibatch accuracy: 91.4%\n",
      "Validation accuracy: 91.2%\n",
      "Minibatch loss at step 200: 0.165754\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 94.5%\n",
      "Test accuracy: 94.5%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 201\n",
    "\n",
    "with tf.Session(graph=graph_CC_LeNet5_v2) as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    print('Initialized')\n",
    "    for step in range(num_steps):\n",
    "        offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "        batch_data = train_dataset[offset:(offset + batch_size), :, :, :]\n",
    "        batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "        feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "        _, l, predictions = session.run(\n",
    "              [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "        if (step % 100 == 0):\n",
    "            print('Minibatch loss at step %d: %f' % (step, l))\n",
    "            print('Minibatch accuracy: %.1f%%' % accuracy(predictions, batch_labels))\n",
    "            print('Validation accuracy: %.1f%%' % accuracy(\n",
    "                valid_prediction.eval(), valid_labels))\n",
    "        \n",
    "    print('Test accuracy: %.1f%%' % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
