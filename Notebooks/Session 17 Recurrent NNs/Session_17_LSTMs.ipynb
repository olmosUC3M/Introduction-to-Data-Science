{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Session 17: Character-based Language Modelling with LSTMs\n",
    "\n",
    "------------------------------------------------------\n",
    "*Introduction to Data Science & Machine Learning*\n",
    "\n",
    "*Pablo M. Olmos olmos@tsc.uc3m.es*\n",
    "\n",
    "------------------------------------------------------\n",
    "\n",
    "The goal of this notebook is to train a LSTM character prediction model over the data base of plain text [Text8](http://mattmahoney.net/dc/textdata).\n",
    "\n",
    "This is a personal wrap-up of all the material provided by [Google's Deep Learning course on Udacity](https://www.udacity.com/course/deep-learning--ud730), so all credit goes to them. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "from __future__ import print_function\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import string\n",
    "import tensorflow as tf\n",
    "import zipfile\n",
    "from six.moves import range\n",
    "from six.moves.urllib.request import urlretrieve\n",
    "\n",
    "#Here I provide some text preprocessing functions\n",
    "import preprocessing as pr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.3.0\n"
     ]
    }
   ],
   "source": [
    "# Lets check what version of tensorflow we have installed. The provided scripts should run with tf 1.0 and above\n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the plain text database from this [link](http://mattmahoney.net/dc/textdata.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = './text8.zip'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the dataset into a single list of characters ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n",
      "Data size 100000000\n"
     ]
    }
   ],
   "source": [
    "text = pr.read_data(filename)\n",
    "print(type(text))\n",
    "print('Data size %d' % len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "v_size = len(string.ascii_lowercase) + 1 #  Number of characters [a-z] + ' '"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The functions char2id and id2char map characters to numeric IDs and back ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 26 0 \n",
      "\n",
      "Unexpected character: ï\n",
      "0 \n",
      "\n",
      "a z  \n"
     ]
    }
   ],
   "source": [
    "print(pr.char2id('a'), pr.char2id('z'), pr.char2id(' '),'\\n')\n",
    "\n",
    "print(pr.char2id('ï'),'\\n')\n",
    "\n",
    "print(pr.id2char(1), pr.id2char(26), pr.id2char(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A RNN-based Character Language Model\n",
    "\n",
    "We will train a RNN with the goal of predicting the next character given current one. The structure is as follows:\n",
    "\n",
    "<img src=\"RNN_CLM.png\" width=\"600\" height=\"400\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a small validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 20 characters of validation set:   anarchism originate \n",
      "\n",
      "\n",
      "First 20 characters of train set:  ions anarchists advo\n"
     ]
    }
   ],
   "source": [
    "valid_size = 1000\n",
    "valid_text = text[:valid_size]\n",
    "train_text = text[valid_size-1:] # The train sequence is the shifted input\n",
    "train_size = len(train_text)\n",
    "\n",
    "print(\"First 20 characters of validation set: \",  valid_text[:20], '\\n\\n')\n",
    "\n",
    "print(\"First 20 characters of train set: \",  train_text[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'BatchGenerator' is a class with a method to generate training batches for the LSTM model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Each element of train_batches.next() is a binary matrix of size  (64, 27)\n",
      "Each sequence is  11  characters long\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64 # Number of sequences in the batch\n",
    "num_unrollings = 10 # Number of characters per sequence\n",
    "\n",
    "\n",
    "train_batches = pr.BatchGenerator(text=train_text, batch_size=batch_size,num_unrollings=num_unrollings,\n",
    "                                  vocabulary_size=v_size)\n",
    "valid_batches = pr.BatchGenerator(text=valid_text, batch_size=1,num_unrollings=1,\n",
    "                                  vocabulary_size=v_size)\n",
    "\n",
    "print('Each element of train_batches.next() is a binary matrix of size ', train_batches.next()[0].shape)\n",
    "\n",
    "print('Each sequence is ',len(train_batches.next()),' characters long')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['cate social', 'nments fail', 'nal park ph', 'ries index ', 'cess of cas', 'r h provide', 'nguage amon', 'ngers in de', 'nal media a', 'e during th', ' known manu', ' seven a wi', 'ss covering', 'een one of ', 'ize single ', 'e first car', 'n in jersey', 'the poverty', 'igns of hum', 'd cause so ', 'in denatura', 'ice formati', ' the input ', 'ick to pull', 'fusion inab', 'complete an', 'st of the m', 'e it fort d', 'attempts by', 'formats for', 'esoteric ch', ' growing po', 'original do', 'ne nine eig', 'arch eight ', 'character l', 'cal mechani', 'n gm compar', 'is fundamen', 'elieve the ', 'east not pa', 'd upon by h', 'm example r', 'sed on the ', 'the officia', 'ion at this', 'ine three t', 'linux enter', 't daily col', 'tration cam', 'nehru wishe', ' stiff from', 'harman s sy', 'to to begin', 'nitiatives ', ' these auth', 'ricky ricar', 'ew of mathe', 'nent of arm', 'ccredited p', 'ne external', 'y other sta', 'l buddhism ', 'evices poss']\n"
     ]
    }
   ],
   "source": [
    "print(pr.batches2string(train_batches.next()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Useful functions to evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def logprob(predictions, labels):\n",
    "    \"\"\"Cross entropy in for the predictions in a batch.\"\"\"\n",
    "    predictions[predictions < 1e-10] = 1e-10\n",
    "    return np.sum(np.multiply(labels, -np.log(predictions))) / labels.shape[0]\n",
    "\n",
    "\n",
    "def sample_distribution(distribution):\n",
    "    \"\"\"Sample one element from a distribution assumed to be an array of normalized\n",
    "        probabilities.\n",
    "    \"\"\"\n",
    "        \n",
    "    r = random.uniform(0,1)\n",
    "    s = 0\n",
    "    for i in range(len(distribution)):\n",
    "        s += distribution[i]\n",
    "        if s >= r:\n",
    "            return i\n",
    "    return len(distribution) - 1\n",
    "\n",
    "\n",
    "def sample(prediction):\n",
    "    \"\"\"Turn a  prediction into 1-hot encoded samples.\"\"\"\n",
    "    p = np.zeros(shape=[1, v_size], dtype=np.float)\n",
    "    p[0, sample_distribution(prediction[0])] = 1.0\n",
    "    return p\n",
    "\n",
    "\n",
    "def random_distribution():\n",
    "    \"\"\"Generate at random a discrete pmf for the characters\"\"\"\n",
    "    b = np.random.uniform(0.0, 1.0, size=[1, v_size])\n",
    "    return b / np.sum(b, 1)[:, None]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM model\n",
    "\n",
    "\n",
    "<img src=\"./files/LSTM_full_4.png\" width=\"800\" height=\"400\">\n",
    "\n",
    "We will use a cross entropy loss function between $\\hat{y}^{(t)}$ and the true one hot labels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "About the TF implementation below, see the following excellent [post](http://www.thushv.com/sequential_modelling/long-short-term-memory-lstm-networks-implementing-with-tensorflow-part-2/)\n",
    "\n",
    "About the zip() and zip(*) operators, see this [post](https://docs.python.org/2/library/functions.html#zip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_nodes = 64\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "  \n",
    "    # Parameters:\n",
    "    \n",
    "    #i(t) parameters\n",
    "    # Input gate: input, previous output, and bias.\n",
    "    ix = tf.Variable(tf.truncated_normal([v_size, num_nodes], -0.1, 0.1))   ##W^ix\n",
    "    im = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1)) ## W^ih\n",
    "    ib = tf.Variable(tf.zeros([1, num_nodes])) ##b_i\n",
    "    \n",
    "    #f(t) parameters\n",
    "    # Forget gate: input, previous output, and bias.\n",
    "    fx = tf.Variable(tf.truncated_normal([v_size, num_nodes], -0.1, 0.1)) ##W^fx\n",
    "    fm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1)) ##W^fh\n",
    "    fb = tf.Variable(tf.zeros([1, num_nodes])) ##b_f\n",
    "    \n",
    "    #g(t) parameters\n",
    "    # Memory cell: input, state and bias.                             \n",
    "    cx = tf.Variable(tf.truncated_normal([v_size, num_nodes], -0.1, 0.1)) ##W^gx\n",
    "    cm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1)) ##W^gh\n",
    "    cb = tf.Variable(tf.zeros([1, num_nodes]))  ##b_g\n",
    "    \n",
    "    #o(t) parameters\n",
    "    # Output gate: input, previous output, and bias.\n",
    "    ox = tf.Variable(tf.truncated_normal([v_size, num_nodes], -0.1, 0.1))  ##W^ox\n",
    "    om = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))  ##W^oh\n",
    "    ob = tf.Variable(tf.zeros([1, num_nodes])) ##b_o\n",
    "    \n",
    "    # Variables saving state across unrollings.\n",
    "    saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False) #h(t)\n",
    "    saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False) #s(t)\n",
    "    \n",
    "    \n",
    "    # Classifier weights and biases (over h(t) to labels)\n",
    "    w = tf.Variable(tf.truncated_normal([num_nodes, v_size], -0.1, 0.1))\n",
    "    b = tf.Variable(tf.zeros([v_size]))\n",
    "  \n",
    "    # Definition of the cell computation.\n",
    "    def lstm_cell(i, o, state):\n",
    "        \n",
    "        \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "        Note that in this formulation, we omit the various connections between the\n",
    "        previous state and the gates.\"\"\"\n",
    "        \n",
    "        input_gate = tf.sigmoid(tf.matmul(i, ix) + tf.matmul(o, im) + ib)\n",
    "        forget_gate = tf.sigmoid(tf.matmul(i, fx) + tf.matmul(o, fm) + fb)\n",
    "        update = tf.matmul(i, cx) + tf.matmul(o, cm) + cb       \n",
    "        state = forget_gate * state + input_gate * tf.tanh(update)    #tf.tanh(update) is g(t)\n",
    "        output_gate = tf.sigmoid(tf.matmul(i, ox) + tf.matmul(o, om) + ob)\n",
    "        return output_gate * tf.tanh(state), state      #h(t) is output_gate * tf.tanh(state)\n",
    "\n",
    "    # Input data is a list of placeholders!\n",
    "    # Train laberls is just the shifted input\n",
    "    \n",
    "    train_data = list()\n",
    "    for _ in range(num_unrollings + 1):\n",
    "        train_data.append(tf.placeholder(tf.float32, shape=[batch_size,v_size]))\n",
    "    train_inputs = train_data[:num_unrollings]\n",
    "    train_labels = train_data[1:]  \n",
    "\n",
    "    # Unrolled LSTM loop\n",
    "    \n",
    "    outputs = list()\n",
    "    output = saved_output\n",
    "    aux = output\n",
    "    state = saved_state\n",
    "    for i in train_inputs:\n",
    "        output, state = lstm_cell(i, output, state)\n",
    "        outputs.append(output)\n",
    "        \n",
    "    \n",
    "    # State saving across unrollings.\n",
    "    with tf.control_dependencies([saved_output.assign(output),saved_state.assign(state)]):\n",
    "        \"\"\" With the tf.control_dependencies we ensure that the logits and the loss are not updated\n",
    "        until all LSTMs outputs are updated.\"\"\"\n",
    "        #Classifier.\n",
    "        logits = tf.nn.xw_plus_b(tf.concat(axis=0,values=outputs), w, b)\n",
    "        loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=tf.concat(axis=0, values=train_labels),logits=logits))\n",
    "\n",
    "    # Optimizer.\n",
    "    \n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=5e-03).minimize(loss) \n",
    "\n",
    "    # Predictions.\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "  \n",
    "    # Sampling and validation eval\n",
    "    sample_input = tf.placeholder(tf.float32, shape=[1, v_size])\n",
    "    \n",
    "    saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    \n",
    "    # Create an op that groups multiple operations.\n",
    "    reset_sample_state = tf.group(saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "                                  saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "    \n",
    "    sample_output, sample_state = lstm_cell(sample_input, saved_sample_output, saved_sample_state)\n",
    "    \n",
    "    with tf.control_dependencies([saved_sample_output.assign(sample_output),saved_sample_state.assign(sample_state)]):\n",
    "        sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: 3.295102\n",
      "Minibatch perplexity: 26.98\n",
      "================================================================================\n",
      "wxbitywrnxbhvnxoqi toarizdv ijcipltstyly ddigukrlxjemkxjhww xvxhcoag rdl ejrqzlk\n",
      "qaztpqqv ykexorvbqwjxroihxrxsscszuxsh cvosgzwdgbssaifzbziueihjrcwjebimfbgjqqcstb\n",
      "jymffuppqyysrxouagrhdcttjovwsyqnzuwogcxnnysfutqxybeqkw fmrzeguepzbchrwnjjyrhinbu\n",
      "oayrh vdiwtcnvjnxduwzqoeglywyqhjkfbmctedieuyttmaarqzqmincjj lsiadskjzzu pyogyqzz\n",
      "gsjokabstrmc tmgzo mtdlvxlmykqjxuinuz tpwtskjturhmgsjlqufgrxsnkhsubtmvjoy zxwuak\n",
      "================================================================================\n",
      "Validation set perplexity: 26.63\n",
      "Average loss at step 100: 2.721629\n",
      "Minibatch perplexity: 10.74\n",
      "Average loss at step 200: 2.295853\n",
      "Minibatch perplexity: 10.13\n",
      "Average loss at step 300: 2.163366\n",
      "Minibatch perplexity: 8.39\n",
      "Average loss at step 400: 2.066477\n",
      "Minibatch perplexity: 7.96\n",
      "Average loss at step 500: 2.001935\n",
      "Minibatch perplexity: 6.90\n",
      "Average loss at step 600: 1.967257\n",
      "Minibatch perplexity: 6.70\n",
      "Average loss at step 700: 1.910152\n",
      "Minibatch perplexity: 5.61\n",
      "Average loss at step 800: 1.872794\n",
      "Minibatch perplexity: 6.40\n",
      "Average loss at step 900: 1.882242\n",
      "Minibatch perplexity: 7.61\n",
      "Average loss at step 1000: 1.876551\n",
      "Minibatch perplexity: 6.34\n",
      "================================================================================\n",
      "ited and popating five sever zero a nermatever wind cymerark genglantion whict t\n",
      "envidgrorm is peatle speptern prock guncendowh puropo s wert theul mil ule dattu\n",
      "vear is an light cumboro from the zero one eigho hive bueze an thind out be dice\n",
      "neank quaschost in the yar jom seven one on emerib averating knov wosh chia cido\n",
      "pon freve gubmekes mhist leade hapseck the seven bofstow and bystions hisse pers\n",
      "================================================================================\n",
      "Validation set perplexity: 6.62\n",
      "Average loss at step 1100: 1.825322\n",
      "Minibatch perplexity: 5.83\n",
      "Average loss at step 1200: 1.797404\n",
      "Minibatch perplexity: 6.01\n",
      "Average loss at step 1300: 1.776419\n",
      "Minibatch perplexity: 5.92\n",
      "Average loss at step 1400: 1.785942\n",
      "Minibatch perplexity: 5.00\n",
      "Average loss at step 1500: 1.786109\n",
      "Minibatch perplexity: 6.45\n",
      "Average loss at step 1600: 1.780660\n",
      "Minibatch perplexity: 5.46\n",
      "Average loss at step 1700: 1.755588\n",
      "Minibatch perplexity: 4.89\n",
      "Average loss at step 1800: 1.711160\n",
      "Minibatch perplexity: 5.59\n",
      "Average loss at step 1900: 1.681630\n",
      "Minibatch perplexity: 6.12\n",
      "Average loss at step 2000: 1.731815\n",
      "Minibatch perplexity: 5.09\n",
      "================================================================================\n",
      "xpubled masked a sead ameritial one nine seieo four with reled nott navies in th\n",
      "a autween to stubcrish in one nine zero one four as the sfimbing states imperour\n",
      "ured time the counts in the illibith wout is the wess it yem endecutiess with co\n",
      "x nest the bahdes roconal propen hiagact populary whisked with thyreed a faxcer \n",
      "jetk firt west nich sours for literanter wer hinda chrugh teams cossaved on dias\n",
      "================================================================================\n",
      "Validation set perplexity: 5.55\n",
      "Average loss at step 2100: 1.719799\n",
      "Minibatch perplexity: 5.28\n",
      "Average loss at step 2200: 1.712476\n",
      "Minibatch perplexity: 5.23\n",
      "Average loss at step 2300: 1.673246\n",
      "Minibatch perplexity: 6.16\n",
      "Average loss at step 2400: 1.686633\n",
      "Minibatch perplexity: 5.42\n",
      "Average loss at step 2500: 1.708455\n",
      "Minibatch perplexity: 6.26\n",
      "Average loss at step 2600: 1.677144\n",
      "Minibatch perplexity: 5.88\n",
      "Average loss at step 2700: 1.685693\n",
      "Minibatch perplexity: 5.30\n",
      "Average loss at step 2800: 1.676008\n",
      "Minibatch perplexity: 5.12\n",
      "Average loss at step 2900: 1.672834\n",
      "Minibatch perplexity: 4.68\n",
      "Average loss at step 3000: 1.669949\n",
      "Minibatch perplexity: 5.13\n",
      "================================================================================\n",
      "ranush bachdne posped gon to they goverablic severaliscos archystlaber is largel\n",
      "pe in the proble dilies from i one eight seven mation the pabridon of coldible t\n",
      "gureds lude one six dividered the suring eschetialic and the player jownvents on\n",
      "d of groiks anmic parl takes its agaicha premated it goncold on amprogunet s def\n",
      "ventory eighttle ruch do is whine four in the gurrable the some there as that ru\n",
      "================================================================================\n",
      "Validation set perplexity: 4.83\n",
      "Average loss at step 3100: 1.649564\n",
      "Minibatch perplexity: 6.07\n",
      "Average loss at step 3200: 1.665646\n",
      "Minibatch perplexity: 4.97\n",
      "Average loss at step 3300: 1.653321\n",
      "Minibatch perplexity: 5.64\n",
      "Average loss at step 3400: 1.682261\n",
      "Minibatch perplexity: 6.18\n",
      "Average loss at step 3500: 1.670252\n",
      "Minibatch perplexity: 5.55\n",
      "Average loss at step 3600: 1.681530\n",
      "Minibatch perplexity: 5.05\n",
      "Average loss at step 3700: 1.659600\n",
      "Minibatch perplexity: 5.38\n",
      "Average loss at step 3800: 1.658830\n",
      "Minibatch perplexity: 4.91\n",
      "Average loss at step 3900: 1.648729\n",
      "Minibatch perplexity: 6.02\n",
      "Average loss at step 4000: 1.661272\n",
      "Minibatch perplexity: 4.51\n",
      "================================================================================\n",
      "elve handia auto elextional redignal do chelata is minding locales and sead e po\n",
      "sidedly debled gordile acce to by fortle roderinas charletims junzer govern and \n",
      "xandest domingties listinits althy can a erimphes and nine space neturus diectra\n",
      "ature which offentesnates of whent soca ofly featured adavers rash areformedbes \n",
      "eesque they hir mikanse regisietpentrics adderrela sol and musine estamy orger p\n",
      "================================================================================\n",
      "Validation set perplexity: 4.67\n",
      "Average loss at step 4100: 1.645561\n",
      "Minibatch perplexity: 5.44\n",
      "Average loss at step 4200: 1.644938\n",
      "Minibatch perplexity: 5.08\n",
      "Average loss at step 4300: 1.627755\n",
      "Minibatch perplexity: 5.26\n",
      "Average loss at step 4400: 1.619514\n",
      "Minibatch perplexity: 4.66\n",
      "Average loss at step 4500: 1.629213\n",
      "Minibatch perplexity: 5.71\n",
      "Average loss at step 4600: 1.626792\n",
      "Minibatch perplexity: 5.20\n",
      "Average loss at step 4700: 1.635892\n",
      "Minibatch perplexity: 5.22\n",
      "Average loss at step 4800: 1.638367\n",
      "Minibatch perplexity: 4.91\n",
      "Average loss at step 4900: 1.644602\n",
      "Minibatch perplexity: 5.21\n",
      "Average loss at step 5000: 1.616386\n",
      "Minibatch perplexity: 5.11\n",
      "================================================================================\n",
      "fs wording they the isson decking unitient place is one five three one hii steve\n",
      "city resustene a history argues her ortanding thy not that sacridst of everazait\n",
      "manses relation a sworks probost is by a britie two three withwhicking craston k\n",
      "jutiazonour betained be compater so if than battredo cullition who twe two four \n",
      "nates zero ziow one zero zero eight one oked thewershiclinate t englished of his\n",
      "================================================================================\n",
      "Validation set perplexity: 4.49\n",
      "Average loss at step 5100: 1.625216\n",
      "Minibatch perplexity: 5.25\n",
      "Average loss at step 5200: 1.612138\n",
      "Minibatch perplexity: 4.61\n",
      "Average loss at step 5300: 1.601359\n",
      "Minibatch perplexity: 5.38\n",
      "Average loss at step 5400: 1.598668\n",
      "Minibatch perplexity: 5.20\n",
      "Average loss at step 5500: 1.586814\n",
      "Minibatch perplexity: 5.39\n",
      "Average loss at step 5600: 1.596494\n",
      "Minibatch perplexity: 4.93\n",
      "Average loss at step 5700: 1.593004\n",
      "Minibatch perplexity: 4.85\n",
      "Average loss at step 5800: 1.603283\n",
      "Minibatch perplexity: 5.06\n",
      "Average loss at step 5900: 1.594179\n",
      "Minibatch perplexity: 4.89\n",
      "Average loss at step 6000: 1.567563\n",
      "Minibatch perplexity: 4.72\n",
      "================================================================================\n",
      "rife or ind probles duigltosely unitquation one sesponatee the worment planding \n",
      "elity of casropthal contembber in to sormet for shoophbas tor of a can marel soo\n",
      "k observed is wo several artoriance chinither marod to remaine hae also he histr\n",
      " conrerming remework euloinat gount three zerooth gelizosia to bank the playclem\n",
      "ve e utok bad forpovers existence japected to bange in acromorito on the one soc\n",
      "================================================================================\n",
      "Validation set perplexity: 4.54\n",
      "Average loss at step 6100: 1.580813\n",
      "Minibatch perplexity: 4.45\n",
      "Average loss at step 6200: 1.553924\n",
      "Minibatch perplexity: 5.60\n",
      "Average loss at step 6300: 1.558161\n",
      "Minibatch perplexity: 4.77\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss at step 6400: 1.555308\n",
      "Minibatch perplexity: 5.31\n",
      "Average loss at step 6500: 1.570216\n",
      "Minibatch perplexity: 4.89\n",
      "Average loss at step 6600: 1.610280\n",
      "Minibatch perplexity: 4.80\n",
      "Average loss at step 6700: 1.588376\n",
      "Minibatch perplexity: 4.70\n",
      "Average loss at step 6800: 1.614277\n",
      "Minibatch perplexity: 5.17\n",
      "Average loss at step 6900: 1.585228\n",
      "Minibatch perplexity: 4.58\n",
      "Average loss at step 7000: 1.587541\n",
      "Minibatch perplexity: 4.79\n",
      "================================================================================\n",
      "lises worls in have was orgining action on continten in the preselds societ one \n",
      "xed im for one nine seven bongia filemor ja regagemal like so is defed the chain\n",
      "querwe bonforth sund filom balania sangdly be officiajiva a chip layer and partu\n",
      "wors every to forbidings part he seem of the gallory but devellations of mill lo\n",
      "peldy the was known typic field beeng in matur os lingu the class the knein when\n",
      "================================================================================\n",
      "Validation set perplexity: 4.45\n",
      "Average loss at step 7100: 1.588127\n",
      "Minibatch perplexity: 4.83\n",
      "Average loss at step 7200: 1.577650\n",
      "Minibatch perplexity: 4.87\n",
      "Average loss at step 7300: 1.573082\n",
      "Minibatch perplexity: 4.22\n",
      "Average loss at step 7400: 1.592044\n",
      "Minibatch perplexity: 5.22\n",
      "Average loss at step 7500: 1.595497\n",
      "Minibatch perplexity: 4.55\n",
      "Average loss at step 7600: 1.551618\n",
      "Minibatch perplexity: 4.08\n",
      "Average loss at step 7700: 1.548614\n",
      "Minibatch perplexity: 4.63\n",
      "Average loss at step 7800: 1.584707\n",
      "Minibatch perplexity: 5.22\n",
      "Average loss at step 7900: 1.583876\n",
      "Minibatch perplexity: 4.65\n",
      "Average loss at step 8000: 1.619163\n",
      "Minibatch perplexity: 5.21\n",
      "================================================================================\n",
      "hetl directly profile in the istaction luthing relative after form moded for fil\n",
      "ctit eughedue number as a uplages hopeanate by cogneved the memolak pretfminding\n",
      "ckeger ppporters be more escistas brien of the diac dyre t fest of asnistic plow\n",
      "berts is over the regrilly are area one nine five advient with one nine seven ea\n",
      "n to denomentery ailstanona comma carly pachay six us and pupper courks of into \n",
      "================================================================================\n",
      "Validation set perplexity: 4.41\n",
      "Average loss at step 8100: 1.595535\n",
      "Minibatch perplexity: 4.35\n",
      "Average loss at step 8200: 1.569083\n",
      "Minibatch perplexity: 5.13\n",
      "Average loss at step 8300: 1.567624\n",
      "Minibatch perplexity: 4.76\n",
      "Average loss at step 8400: 1.576961\n",
      "Minibatch perplexity: 5.27\n",
      "Average loss at step 8500: 1.579742\n",
      "Minibatch perplexity: 4.87\n",
      "Average loss at step 8600: 1.573724\n",
      "Minibatch perplexity: 4.42\n",
      "Average loss at step 8700: 1.565836\n",
      "Minibatch perplexity: 4.60\n",
      "Average loss at step 8800: 1.539117\n",
      "Minibatch perplexity: 4.77\n",
      "Average loss at step 8900: 1.558318\n",
      "Minibatch perplexity: 4.52\n",
      "Average loss at step 9000: 1.551076\n",
      "Minibatch perplexity: 4.64\n",
      "================================================================================\n",
      "on with and bafer wall be manies in the using a parmary palad in one two zero wa\n",
      "w or an interrebed our and populations of enligy the fushing one nine from have \n",
      "cons on a factorion and influent proposider have faith the paid a musite sbved i\n",
      "x a selbanns growed latener a massosth import till one nine one five s weokdouth\n",
      "us and the kirh in one seven yixt on it more as a thort mowen priered v orders i\n",
      "================================================================================\n",
      "Validation set perplexity: 4.43\n",
      "Average loss at step 9100: 1.563074\n",
      "Minibatch perplexity: 4.43\n",
      "Average loss at step 9200: 1.584966\n",
      "Minibatch perplexity: 4.28\n",
      "Average loss at step 9300: 1.594449\n",
      "Minibatch perplexity: 5.98\n",
      "Average loss at step 9400: 1.580636\n",
      "Minibatch perplexity: 5.21\n",
      "Average loss at step 9500: 1.586747\n",
      "Minibatch perplexity: 4.90\n",
      "Average loss at step 9600: 1.569876\n",
      "Minibatch perplexity: 4.82\n",
      "Average loss at step 9700: 1.585649\n",
      "Minibatch perplexity: 4.73\n",
      "Average loss at step 9800: 1.589346\n",
      "Minibatch perplexity: 5.21\n",
      "Average loss at step 9900: 1.589253\n",
      "Minibatch perplexity: 5.38\n",
      "Average loss at step 10000: 1.597899\n",
      "Minibatch perplexity: 4.52\n",
      "================================================================================\n",
      "k sesures gamet durded were the end bevow one nine one nine zero lastern other a\n",
      "planslu the asains to relian the reafilism later pressert and several apkin an a\n",
      "ox discovlan certabus avaimantic internet of the place frapork of goedons artist\n",
      "aru used where to callime and rocker publilel bingen are armilam show have the l\n",
      "negophesis sany etherming ashay boungare after three number but methes francus a\n",
      "================================================================================\n",
      "Validation set perplexity: 4.40\n"
     ]
    }
   ],
   "source": [
    "num_steps = 10001\n",
    "summary_frequency = 100\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    print('Initialized')\n",
    "    mean_loss = 0\n",
    "    for step in range(num_steps):\n",
    "        batches = train_batches.next()\n",
    "        feed_dict = dict()\n",
    "        \n",
    "        # List of input placeholders\n",
    "        for i in range(num_unrollings + 1):\n",
    "            feed_dict[train_data[i]] = batches[i]\n",
    "            \n",
    "        # We run the optimizer    \n",
    "        _, l, predictions = session.run([optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "        \n",
    "        mean_loss += l\n",
    "        if step % summary_frequency == 0:\n",
    "            \n",
    "            if step > 0:\n",
    "                mean_loss /= summary_frequency\n",
    "                \n",
    "            # The mean loss is an estimate of the loss over the last few batches.\n",
    "            print('Average loss at step %d: %f' % (step, mean_loss))\n",
    "            \n",
    "            mean_loss = 0\n",
    "            \n",
    "            labels = np.concatenate(list(batches)[1:])\n",
    "            \n",
    "            # The perplexity is simply the exponential cross entropy. The smaller the better\n",
    "            print('Minibatch perplexity: %.2f' % float(np.exp(logprob(predictions, labels))))\n",
    "            \n",
    "            if step % (summary_frequency * 10) == 0:\n",
    "                # Generate some samples.\n",
    "                print('=' * 80)\n",
    "                for _ in range(5):\n",
    "                    \n",
    "                    feed = sample(random_distribution())\n",
    "                    sentence = pr.characters(feed)[0] #one-hote encodding \n",
    "                    reset_sample_state.run()\n",
    "                    \n",
    "                    # We sample one by one\n",
    "                    for _ in range(79):\n",
    "                        prediction = sample_prediction.eval({sample_input: feed})\n",
    "                        feed = sample(prediction)\n",
    "                        sentence += pr.characters(feed)[0]\n",
    "                    print(sentence)\n",
    "                print('=' * 80)\n",
    "                \n",
    "                # Measure validation set perplexity.\n",
    "                reset_sample_state.run()\n",
    "                valid_logprob = 0\n",
    "\n",
    "                for _ in range(valid_size):\n",
    "                    b = valid_batches.next()\n",
    "                    predictions = sample_prediction.eval({sample_input: b[0]})\n",
    "                    valid_logprob = valid_logprob + logprob(predictions, b[1])\n",
    "                    \n",
    "                print('Validation set perplexity: %.2f' % float(np.exp(valid_logprob / valid_size)))\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
